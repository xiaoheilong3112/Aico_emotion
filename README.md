# AICO æƒ…æ„Ÿç³»ç»Ÿ

> åŸºäº VAD æ¨¡å‹çš„åŒè½®è¶³æœºå™¨äººæƒ…æ„Ÿè®¡ç®—ç³»ç»Ÿ

## ğŸ‰ æœ€æ–°è¿›å±•

**ğŸ“… 2026å¹´1æœˆ15æ—¥ v1.4** - ç®€åŒ–é¢æ¿ + SQLiteæ•°æ®åº“ï¼
- âœ… **ç®€åŒ–å¯è§†åŒ–**: 280pxç²¾ç®€é¢æ¿ï¼Œåªæ˜¾ç¤ºæ ¸å¿ƒä¿¡æ¯ï¼ˆæƒ…æ„Ÿã€ç½®ä¿¡åº¦ã€è¾¹ç•Œæ¡†ï¼‰
- âœ… **æ•°æ®åº“é›†æˆ**: SQLiteå­˜å‚¨è¯¦ç»†ä¿¡æ¯ï¼ˆVADå€¼ã€æƒ…æ„Ÿåˆ†å¸ƒã€æŠ€æœ¯å‚æ•°ï¼‰
- âœ… **æ•°æ®æŸ¥è¯¢**: æä¾›query_database.pyå·¥å…·æŸ¥çœ‹ç»Ÿè®¡å’Œè¯¦æƒ…
- âœ… **æ–‡ä»¶ç®¡ç†**: æ‰€æœ‰æµ‹è¯•æ–‡ä»¶ç»Ÿä¸€æ”¾åœ¨tests/ç›®å½•
- âœ… **æ€§èƒ½ä¼˜åŒ–**: å›¾ç‰‡ä½“ç§¯å‡å°‘15-25%ï¼ˆ17-20KB vs 22-24KBï¼‰
- ğŸ“Š æ•°æ®åº“: test_outputs/emotion_detection.db
- ğŸ“ æŸ¥è¯¢å·¥å…·: tests/query_database.py

## å¿«é€Ÿå¼€å§‹

### 1. åˆå§‹åŒ–é¡¹ç›®

```bash
# å…‹éš†æˆ–è¿›å…¥é¡¹ç›®ç›®å½•
cd Aico_emotion

# è¿è¡Œåˆå§‹åŒ–è„šæœ¬ï¼ˆè‡ªåŠ¨åˆ›å»ºç¯å¢ƒå’Œå®‰è£…ä¾èµ–ï¼‰
bash setup_project.sh
```

### 2. ä¸‹è½½MediaPipeæ¨¡å‹ï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
# ä¸‹è½½Googleå®˜æ–¹äººè„¸æ£€æµ‹æ¨¡å‹
bash download_mediapipe_models.sh
```

### 3. æ¿€æ´»å¼€å‘ç¯å¢ƒ

```bash
source .venv/bin/activate
```

### 4. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
bash run_tests.sh

# æˆ–æ‰‹åŠ¨è¿è¡Œç‰¹å®šæµ‹è¯•
uv run pytest tests/test_state.py -v
```

### 5. è§†è§‰æ£€æµ‹æ¼”ç¤º

```bash
# å®æ—¶æ‘„åƒå¤´æ£€æµ‹
uv run python demo_vision.py realtime

# å›¾ç‰‡æ–‡ä»¶æ£€æµ‹
uv run python demo_vision.py image

# å¯¹æ¯”ä¸åŒæ£€æµ‹å™¨æ•ˆæœ
uv run python demo_vision.py compare

# å¯è§†åŒ–æµ‹è¯•ï¼ˆAffectNetæ•°æ®é›†ï¼Œä¿å­˜åˆ°æ•°æ®åº“ï¼‰
uv run python tests/test_real_images.py vis sad 3
```

### 6. æŸ¥è¯¢æ£€æµ‹æ•°æ®åº“

```bash
# æŸ¥çœ‹æœ€è¿‘çš„æ£€æµ‹è®°å½•
uv run python tests/query_database.py recent 10

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
uv run python tests/query_database.py stats

# æŸ¥çœ‹ç‰¹å®šæƒ…æ„Ÿç»Ÿè®¡
uv run python tests/query_database.py stats fear

# æŸ¥çœ‹æ£€æµ‹è¯¦æƒ…ï¼ˆå«VADå€¼å’Œæƒ…æ„Ÿåˆ†å¸ƒï¼‰
uv run python tests/query_database.py detail 1

# å¯¼å‡ºæ•°æ®åˆ°JSON
uv run python tests/query_database.py export output.json 100
```

### 7. å¯åŠ¨ä¸»ç¨‹åº

```bash
# ä»…è§†è§‰æ„ŸçŸ¥ï¼ˆéœ€è¦æ‘„åƒå¤´ï¼‰
uv run python main.py --vision

# å¤šæ¨¡æ€æ„ŸçŸ¥
uv run python main.py --vision --audio

# æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
uv run python main.py --help
```

## é¡¹ç›®ç»“æ„

```
Aico_emotion/
â”œâ”€â”€ config/              # äººæ ¼é…ç½®æ–‡ä»¶
â”œâ”€â”€ src/                 # æºä»£ç 
â”‚   â”œâ”€â”€ affect/          # æƒ…æ„Ÿæ ¸å¿ƒ âœ…
â”‚   â”‚   â”œâ”€â”€ state.py     # VADçŠ¶æ€ã€Perceptæ•°æ®ç»“æ„
â”‚   â”‚   â””â”€â”€ personality.py  # äººæ ¼é…ç½®åŠ è½½å™¨
â”‚   â”œâ”€â”€ perception/      # æ„ŸçŸ¥æ¨¡å— âœ…
â”‚   â”‚   â””â”€â”€ vision.py    # è§†è§‰æƒ…æ„Ÿè¯†åˆ« (MediaPipe + FER)
â”‚   â”œâ”€â”€ policy/          # ç­–ç•¥å¼•æ“ â­ï¸
â”‚   â”œâ”€â”€ expression/      # è¡¨è¾¾å±‚ â­ï¸
â”‚   â””â”€â”€ utils/           # å·¥å…·æ¨¡å— âœ…
â”‚       â””â”€â”€ database.py  # æƒ…æ„Ÿæ£€æµ‹ç»“æœæ•°æ®åº“
â”œâ”€â”€ tests/               # æµ‹è¯•ä»£ç  âœ…
â”‚   â”œâ”€â”€ test_state.py         # çŠ¶æ€æ¨¡å—æµ‹è¯•
â”‚   â”œâ”€â”€ test_personality.py   # äººæ ¼é…ç½®æµ‹è¯•
â”‚   â”œâ”€â”€ test_vision.py        # è§†è§‰æ„ŸçŸ¥æµ‹è¯•
â”‚   â”œâ”€â”€ test_real_images.py   # çœŸå®å›¾åƒæµ‹è¯•ï¼ˆå«æ•°æ®åº“é›†æˆï¼‰
â”‚   â””â”€â”€ query_database.py     # æ•°æ®åº“æŸ¥è¯¢å·¥å…·
â”œâ”€â”€ test_outputs/        # æµ‹è¯•è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ visualizations/       # å¯è§†åŒ–æ ‡æ³¨å›¾ç‰‡ï¼ˆ280pxç®€åŒ–é¢æ¿ï¼‰
â”‚   â”œâ”€â”€ reports/              # æµ‹è¯•æŠ¥å‘Š
â”‚   â””â”€â”€ emotion_detection.db  # SQLiteæ•°æ®åº“ï¼ˆè¯¦ç»†ä¿¡æ¯ï¼‰
â”œâ”€â”€ models/              # AIæ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ blaze_face_short_range.tflite  # MediaPipeäººè„¸æ£€æµ‹
â”‚   â”œâ”€â”€ haarcascade_frontalface_default.xml
â”‚   â””â”€â”€ EmotionDetectionModel.h5       # FERæƒ…æ„Ÿæ¨¡å‹
â”œâ”€â”€ demo_vision.py       # è§†è§‰æ£€æµ‹æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ test_real_images.py  # AffectNetçœŸå®å›¾åƒæµ‹è¯•è„šæœ¬
â”œâ”€â”€ main.py              # ä¸»ç¨‹åºå…¥å£ â­ï¸
â””â”€â”€ pyproject.toml       # é¡¹ç›®é…ç½®

âœ… å·²å®Œæˆ  ğŸš§ è¿›è¡Œä¸­  â­ï¸ å¾…å¼€å‘
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### è§†è§‰æƒ…æ„Ÿè¯†åˆ«

```python
from src.perception.vision import VisionPerceptor, detect_emotion_from_image

# æ–¹å¼1: ä»æ‘„åƒå¤´å®æ—¶æ£€æµ‹
with VisionPerceptor(camera_id=0) as vp:
    percept = vp.perceive()
    if percept:
        print(f"æƒ…æ„Ÿ: {percept.metadata['dominant_emotion']}")
        print(f"VAD: V={percept.valence_hint:.2f}, A={percept.arousal_hint:.2f}")

# æ–¹å¼2: ä»å›¾ç‰‡æ–‡ä»¶æ£€æµ‹
percept = detect_emotion_from_image("face.jpg")
if percept:
    print(f"æ£€æµ‹åˆ° {percept.confidence:.0%} ç½®ä¿¡åº¦çš„ {percept.metadata['dominant_emotion']} æƒ…æ„Ÿ")

# æ–¹å¼3: æ‰¹é‡å¤„ç†
vp = VisionPerceptor(camera_id=0)
frames = [...]  # å›¾åƒåˆ—è¡¨
results = vp.batch_perceive(frames)
```

### æµ‹è¯•çœŸå®å›¾åƒï¼ˆAffectNetæ•°æ®é›†ï¼‰

```bash
# æµ‹è¯•æ‰€æœ‰æƒ…æ„Ÿç±»åˆ«ï¼ˆæ¯ç±»5å¼ æ ·æœ¬ï¼‰
uv run python test_real_images.py

# æµ‹è¯•å•ä¸ªç±»åˆ«ï¼ˆå¦‚happyï¼Œ10å¼ æ ·æœ¬ï¼‰
uv run python test_real_images.py single happy 10

# å¯è§†åŒ–ç»“æœï¼ˆç”Ÿæˆæ ‡æ³¨å›¾ç‰‡ï¼‰
uv run python test_real_images.py vis fear
```

## æ ¸å¿ƒæ¦‚å¿µ

### VAD æƒ…æ„Ÿç©ºé—´

- **Valenceï¼ˆæ„‰æ‚¦åº¦ï¼‰**ï¼š[-1, 1] ä¸æ„‰å¿« â†’ æ„‰å¿«
- **Arousalï¼ˆæ¿€æ´»åº¦ï¼‰**ï¼š[0, 1] å¹³é™ â†’ æ¿€åŠ¨
- **Dominanceï¼ˆä¸»å¯¼åº¦ï¼‰**ï¼š[-1, 1] è¢«åŠ¨ â†’ ä¸»åŠ¨

### æ¨¡å—æ¶æ„

```
æ„ŸçŸ¥å±‚ â†’ æ¨æ–­å±‚ â†’ çŠ¶æ€å±‚ â†’ ç­–ç•¥å±‚ â†’ è¡¨è¾¾å±‚
```

## å¼€å‘æŒ‡å—

è¯¦ç»†å¼€å‘æ–¹æ¡ˆè¯·å‚è€ƒï¼š[AICO_æƒ…æ„Ÿç³»ç»Ÿå¼€å‘æ–¹æ¡ˆ_æ‰§è¡Œç‰ˆ.md](AICO_æƒ…æ„Ÿç³»ç»Ÿå¼€å‘æ–¹æ¡ˆ_æ‰§è¡Œç‰ˆ.md)

### æ·»åŠ æ–°çš„æ„ŸçŸ¥æ¨¡æ€

1. åœ¨ `src/perception/` åˆ›å»ºæ–°æ¨¡å—
2. å®ç° `perceive()` æ–¹æ³•ï¼Œè¿”å› `Percept` å¯¹è±¡
3. åœ¨ `main.py` ä¸­æ³¨å†Œ

### è‡ªå®šä¹‰äººæ ¼

ç¼–è¾‘ `config/personality.yaml`ï¼š

```yaml
personality:
  emotional_gain: 0.7      # è°ƒæ•´æƒ…ç»ªæ•æ„Ÿåº¦
  recovery_rate: 0.01      # è°ƒæ•´æ¢å¤é€Ÿåº¦
  expressiveness: 0.8      # è°ƒæ•´è¡¨è¾¾å¼ºåº¦
```

### è¿è¡Œæ€§èƒ½æµ‹è¯•

```bash
uv run python scripts/benchmark.py
```

## éƒ¨ç½²åˆ° RK3588

```bash
# æ„å»º Docker é•œåƒ
docker build -t aico-emotion:v1 .

# è¿è¡Œï¼ˆå¸¦è®¾å¤‡è®¿é—®ï¼‰
docker run --rm -it \
  --device /dev/video0 \
  --device /dev/snd \
  -v $(pwd)/config:/app/config \
  aico-emotion:v1
```

## ä¾èµ–è¯´æ˜

- **OpenCV**ï¼šå›¾åƒå¤„ç†
- **MediaPipe**ï¼šäººè„¸æ£€æµ‹
- **FER**ï¼šè¡¨æƒ…è¯†åˆ«
- **SpeechRecognition**ï¼šè¯­éŸ³è¯†åˆ«
- **Transformers**ï¼šNLP æƒ…æ„Ÿåˆ†æ

## æ•…éšœæ’é™¤

### æ‘„åƒå¤´æ— æ³•è®¿é—®

```bash
# æ£€æŸ¥è®¾å¤‡
ls /dev/video*

# æµ‹è¯•æ‘„åƒå¤´
uv run python -c "import cv2; cap = cv2.VideoCapture(0); print(cap.isOpened())"
```

### éº¦å…‹é£æ— æ³•è¯†åˆ«

```bash
# æ£€æŸ¥éŸ³é¢‘è®¾å¤‡
arecord -l

# æµ‹è¯•å½•éŸ³
arecord -d 3 test.wav
```

### ä¾èµ–å®‰è£…å¤±è´¥

```bash
# å®‰è£…ç³»ç»Ÿä¾èµ–ï¼ˆUbuntuï¼‰
sudo apt-get install -y \
  python3.10-dev \
  portaudio19-dev \
  libopencv-dev \
  ffmpeg

# é‡æ–°å®‰è£… Python ä¾èµ–
uv pip install --force-reinstall -e .
```

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µï¼šå¾…å®š
- é—®é¢˜åé¦ˆï¼šGitHub Issues
